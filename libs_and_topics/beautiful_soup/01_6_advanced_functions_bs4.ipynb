{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "\n",
    "source = requests.get('http://coreyms.com').text\n",
    "\n",
    "soup = BeautifulSoup(source, 'lxml')\n",
    "\n",
    "csv_file = open('cms_scrape.csv', 'w')\n",
    "\n",
    "csv_writer = csv.writer(csv_file)\n",
    "csv_writer.writerow(['headline', 'summary', 'video_link'])\n",
    "\n",
    "for article in soup.find_all('article'):\n",
    "    headline = article.h2.a.text\n",
    "    print(headline)\n",
    "\n",
    "    summary = article.find('div', class_='entry-content').p.text\n",
    "    print(summary)\n",
    "\n",
    "    try:\n",
    "        vid_src = article.find('iframe', class_='youtube-player')['src']\n",
    "\n",
    "        vid_id = vid_src.split('/')[4]\n",
    "        vid_id = vid_id.split('?')[0]\n",
    "\n",
    "        yt_link = f'https://youtube.com/watch?v={vid_id}'\n",
    "    except Exception as e:\n",
    "        yt_link = None\n",
    "\n",
    "    print(yt_link)\n",
    "\n",
    "    print()\n",
    "\n",
    "    csv_writer.writerow([headline, summary, yt_link])\n",
    "\n",
    "csv_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple pages\n",
    "# there are 13 pages\n",
    "# go to next one and check the url\n",
    "# http://www.espn.com/nba/statistics/rpm/_/sort/RPM\n",
    "# page 2 - http://www.espn.com/nba/statistics/rpm/_/page/2\n",
    "# check if page 2 syntax applies to the first one\n",
    "# http://www.espn.com/nba/statistics/rpm/_/page/1\n",
    "# it does. In case it doesn't you need to scrape them separately\n",
    "# we need to iterate from 1 to 13\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "user_headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "# create a counter\n",
    "number = 1\n",
    "\n",
    "# create a file that will be used for writing\n",
    "# call in write mode\n",
    "with open ('basketball_stats_table_part3.txt', 'w') as main_document:\n",
    "    # create a header and a new line\n",
    "    main_document.write(\" Basketball Stats\\n\")\n",
    "\n",
    "\n",
    "# create a loop\n",
    "while number < 14:\n",
    "    # format url to include counter\n",
    "    url = 'http://www.espn.com/nba/statistics/rpm/_/page/{}'.format(number)\n",
    "    \n",
    "    \n",
    "    # otional but recommended, delay the execution of every cicle \n",
    "    # helps website performance\n",
    "    # wait 1 second\n",
    "    time.sleep(1)\n",
    "    \n",
    "    #get response\n",
    "    response = requests.get(url, headers = user_headers)\n",
    "    \n",
    "    # ensure 200 response \n",
    "    if response.status_code == 200:\n",
    "        # parse the soup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # get table\n",
    "        stat_table = soup.find_all('table', class_ = \"tablehead\")\n",
    "        # check if there's only one table in the page\n",
    "        if len(stat_table) < 2:\n",
    "            # get first iteration\n",
    "            stat_table = stat_table[0]\n",
    "            # open append object using 'a'\n",
    "            # write object would rewrite the file each iteration\n",
    "            with open ('basketball_stats_table_part3.txt', 'a') as document:\n",
    "                # iterate as part1\n",
    "                for row in stat_table.find_all('tr'):\n",
    "                        for cell in row.find_all('td'):\n",
    "                            document.write(cell.text.ljust(30))\n",
    "                        document.write('\\n')\n",
    "        else:\n",
    "            print(\"too many tables\")\n",
    "    else:\n",
    "        print('No response')\n",
    "        print(number)\n",
    "        \n",
    "    # increase counter\n",
    "    number += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## japan example - stop iterating when you reach some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find Vita's upcoming releases\n",
    "# get response\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://gamefaqs.gamespot.com/pc'\n",
    "response = requests.get(url, headers = {'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "response.status_code\n",
    "\n",
    "# make soup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "soup.find(string = 'Japan')\n",
    "\n",
    "# find parents\n",
    "soup.find(string = 'Japan').find_all_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of instances\n",
    "len(soup.find(string = 'Japan').find_all_next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improve readability\n",
    "for item in range(11):\n",
    "    print(soup.find(string = 'Japan').find_all_next())\n",
    "    # print new line and asteriscs after each instance\n",
    "    print('\\n*****************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h3 holds headers for each region\n",
    "soup.select('h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get japan by slicing\n",
    "soup.select('h3')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find next to get date\n",
    "soup.select('h3')[2].find_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go deeper 1 level\n",
    "soup.select('h3')[2].find_next().find_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go deeper 1 level\n",
    "soup.select('h3')[2].find_next().find_next().find_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next iteration gives the titles\n",
    "soup.select('h3')[2].find_next().find_next().find_next().find_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get type\n",
    "type(soup.select('h3')[2].find_next().find_next().find_next().find_next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# japan ends where europe starts\n",
    "\n",
    "# create list\n",
    "append_list = []\n",
    "\n",
    "# get slice for japan \n",
    "# store in variable\n",
    "japan_header = soup.select('h3')[2]\n",
    "\n",
    "# iterate until you get to Europe which is our keyword to stop iterating \n",
    "while str(japan_header) != '<h3>Europe</h3>':\n",
    "    # find next element\n",
    "    japan_header = japan_header.find_next()\n",
    "    # append the text of the element iteration\n",
    "    append_list.append(japan_header.get_text())\n",
    "    \n",
    "# print list\n",
    "append_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean list\n",
    "from collections import OrderedDict\n",
    "\n",
    "# create list with list\n",
    "# order list with OrderedDict\n",
    "list(OrderedDict.fromkeys(append_list))\n",
    "\n",
    "# iterate list to remove repetition\n",
    "# remove last 2 items with [:-2]\n",
    "for item in list(OrderedDict.fromkeys(append_list))[:-2]:\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
