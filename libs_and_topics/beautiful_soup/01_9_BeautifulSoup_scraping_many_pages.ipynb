{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scraping many pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# many pages with same url\n",
    "# https://stackoverflow.com/questions/26497722/scrape-multiple-pages-with-beautifulsoup-and-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are 13 pages\n",
    "# go to next one and check the url\n",
    "# http://www.espn.com/nba/statistics/rpm/_/sort/RPM\n",
    "# page 2 - http://www.espn.com/nba/statistics/rpm/_/page/2\n",
    "# check if page 2 syntax applies to the first one\n",
    "# http://www.espn.com/nba/statistics/rpm/_/page/1\n",
    "# it does. In case it doesn't you need to scrape them separately\n",
    "# we need to iterate from 1 to 13\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "user_headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "# create a counter\n",
    "number = 1\n",
    "\n",
    "# create a file that will be used for writing\n",
    "# call in write mode\n",
    "with open ('basketball_stats_table_part3.txt', 'w') as main_document:\n",
    "    # create a header and a new line\n",
    "    main_document.write(\" Basketball Stats\\n\")\n",
    "\n",
    "\n",
    "# create a loop\n",
    "while number < 14:\n",
    "    # format url to include counter\n",
    "    url = 'http://www.espn.com/nba/statistics/rpm/_/page/{}'.format(number)\n",
    "    \n",
    "    \n",
    "    # otional but recommended, delay the execution of every cicle \n",
    "    # helps website performance\n",
    "    # wait 1 second\n",
    "    time.sleep(1)\n",
    "    \n",
    "    #get response\n",
    "    response = requests.get(url, headers = user_headers)\n",
    "    \n",
    "    # ensure 200 response \n",
    "    if response.status_code == 200:\n",
    "        # parse the soup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # get table\n",
    "        stat_table = soup.find_all('table', class_ = \"tablehead\")\n",
    "        # check if there's only one table in the page\n",
    "        if len(stat_table) < 2:\n",
    "            # get first iteration\n",
    "            stat_table = stat_table[0]\n",
    "            # open append object using 'a'\n",
    "            # write object would rewrite the file each iteration\n",
    "            with open ('basketball_stats_table_part3.txt', 'a') as document:\n",
    "                # iterate as part1\n",
    "                for row in stat_table.find_all('tr'):\n",
    "                        for cell in row.find_all('td'):\n",
    "                            document.write(cell.text.ljust(30))\n",
    "                        document.write('\\n')\n",
    "        else:\n",
    "            print(\"too many tables\")\n",
    "    else:\n",
    "        print('No response')\n",
    "        print(number)\n",
    "        \n",
    "    # increase counter\n",
    "    number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
